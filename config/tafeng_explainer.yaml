# How many hops for the subgraph? (Only used if `user_batch_exp` == 1)
n_hops: 2
# `n_hops` is doubled such that each hop considers all the interactions of closest users in the bipartite graph
neighbors_hops: True
# optimizer for explainer
cf_optimizer: SGD
# learning rate of optimizer for explainer
cf_learning_rate: 3000 # 1200 for NDCGApprox
# set to update weights after each mini-batch, otherwise after a full batch
mini_batch_descent: True

momentum: 0.0
# epochs to generate explanations
cf_epochs: 800
# how many users should be considered to explain?
user_batch_exp: 256
# not consider the prediction loss
not_pred: True
# if the fair or pred loss should be deactivated if the top-k list is already perturbed
pred_same: False
# loss weight for the graph dist loss
cf_beta: 0.5
# the function used to check if the top-k list was perturbed
cf_dist: damerau_levenshtein
# how many top items to consider for model outcome list and fairness loss
cf_topk: 10

# metric used for demographic parity loss and debugging
metric_loss: ndcg

# loss weight for the fairness loss
fair_beta: 0.5
dropout_prob: 0.3

edge_additions: False

# select which set need to be used to optimize the DP explainer. Choices ["train", "valid", "test"]
exp_rec_data: "test"

# device: cpu

# "local": the optimization uses the NDCG of the disadvantaged group computed locally for the current batch
# "global": the optimization uses the NDCG of the disadvantaged group computed globally on the original model
only_adv_group: "local"
# Choose if the edges of the advantaged group should be deleted or the opposite
delete_adv_group: True

# Last.FM 1K
# data_path: 'src/dataset/lastfm-1k' # for Last.FM 1K
# ML-1M
# data_path: 'dataset/ml-1m' # for ML-1M

# consider only the nodes at the last hop
sub_matrix_only_last_level: False
# does not consider the interactions of the user to explain, consider the remaining hops
not_user_sub_matrix: False
# does not perform perturbation, uses only the subgraph of the user to `explain` (used for analysis)
only_subgraph: False

# sensitive attributes to be used in the fairness losses and for following analysis
sensitive_attribute: age

# if True each new exp will have a different number of edges from the last one
save_unique_graph_dist_loss: True

# force return of explanations even though the top-k is not perturbed (only useful for hops analysis)
explainer_force_return: False

# ML-100K
load_col:
    inter: [customer_id, product_id]
    user: [customer_id, age, residence_area_code]
    item: [product_id, product_subclass]

eval_args:
    split: {'LRS': None}
    order: RO  # not relevant
    group_by: '-'
    mode: 'full'

# Early stopping parameters
early_stopping:
    patience: 40  # a periodicity is visible on the charts, with low peaks better than the previous ones after 20-40 epochs
    ignore: 0
    method: 'consistency'
    mode: 'lt'  # lower than, use any other python func ('le', 'gt', 'ge')
    delta: 0.001  # several experiments show many low peaks with differences about > 0.15, which are relevant for us

previous_loss_value: False
previous_batch_LR_scaling: False

random_perturbation_p: 0.000014

# use 'random' to initialize it randomly around a fixed value, otherwise the fixed value will be used
perturbation_initialization: 'static'

# Policies
explainer_policies:
    increase_disparity: False
    force_removed_edges: True
    group_deletion_constraint: False   # this and `random_perturbation` cannot be both True
    random_perturbation: False
    neighborhood_perturbation: False   # the perturbation spread from the first perturbed edges towards the neighbors
